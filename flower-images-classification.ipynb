{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/siddp6/flower-images-classification?scriptVersionId=139093022\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Crafting an AI Application\n\nThe future sees AI algorithms integrated into everyday applications. For instance, think of an image classifier in a smartphone app, powered by a deep learning model trained on vast image datasets. This integration of models into applications will become a staple in software development.\n\nThis project focuses on training an image classifier to recognize flower species. Imagine a camera app identifying flowers. We'll use a dataset of [102 flower categories](https://www.kaggle.com/datasets/siddp6/flower-dataset). The project involves:\n\n- Loading and preprocessing the image dataset\n- Training the classifier\n- Using it for image predictions\n\nEach step will be guided, implemented in Python.\n\nThis project will equip you to build classifiers for any labeled image set. Apply these skills to create innovative applications, like identifying car models from pictures. Let's start by importing required packages, adhering to the practice of placing imports at the code's outset.\n","metadata":{}},{"cell_type":"code","source":"# Importing required packages\n\nimport os  # Operating system functions\nimport json  # For working with JSON files\nimport torch  # PyTorch library\n\nimport torch.nn.functional as F  # Functional module of PyTorch's neural network\nimport numpy as np  # Numerical computing library\nimport matplotlib.pyplot as plt  # Plotting library\n\nfrom torch import nn  # Neural network module of PyTorch\nfrom torch import optim  # Optimization algorithms\nfrom collections import OrderedDict  # For creating ordered dictionaries\nfrom torchvision import datasets, transforms, models  # PyTorch's vision library\nfrom PIL import Image  # Python Imaging Library","metadata":{"execution":{"iopub.status.busy":"2023-08-06T15:13:32.247368Z","iopub.execute_input":"2023-08-06T15:13:32.247663Z","iopub.status.idle":"2023-08-06T15:13:36.739103Z","shell.execute_reply.started":"2023-08-06T15:13:32.247633Z","shell.execute_reply":"2023-08-06T15:13:36.737977Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Set up matplotlib for inline plotting with high-resolution figures\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'","metadata":{"execution":{"iopub.status.busy":"2023-08-06T15:13:36.740779Z","iopub.execute_input":"2023-08-06T15:13:36.741459Z","iopub.status.idle":"2023-08-06T15:13:36.770821Z","shell.execute_reply.started":"2023-08-06T15:13:36.741428Z","shell.execute_reply":"2023-08-06T15:13:36.768306Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Hyperparameters\nbatch_size = 64  # Number of samples per batch during training\nepochs = 10  # Number of times to iterate over the entire dataset\nlearning_rate = 0.001  # Learning rate for updating model parameters\ncriterion = nn.NLLLoss()  # Negative Log Likelihood loss\n\n# Other constants\ninput_size = 25088  # Input size of the network (flattened image size)\noutput_size = 102  # Number of possible output classes (flower species)\nhidden_layers = 512  # Number of units in the hidden layers\ndrop_rate = 0.2  # Probability of dropout during training\n\n# Device\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # Using GPU if available, else CPU","metadata":{"execution":{"iopub.status.busy":"2023-08-06T15:14:28.054017Z","iopub.execute_input":"2023-08-06T15:14:28.054377Z","iopub.status.idle":"2023-08-06T15:14:28.083856Z","shell.execute_reply.started":"2023-08-06T15:14:28.054347Z","shell.execute_reply":"2023-08-06T15:14:28.082584Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Loading the Data\n\nIn this section, you will utilize `torchvision` to load the data. The dataset is provided either alongside this notebook or can be [downloaded here](https://s3.amazonaws.com/content.udacity-data.com/nd089/flower_data.tar.gz). It's divided into three main parts: training, validation, and testing.\n\nFor the training data, various transformations will be applied, including random scaling, cropping, and flipping. These transformations enhance the network's ability to generalize and improve its overall performance. Moreover, it's crucial to resize the input data to 224x224 pixels to comply with the requirements of the pre-trained networks.\n\nThe validation and testing sets play a role in assessing the model's performance on unseen data. Consequently, no scaling or rotation transformations are applied here. Instead, images need to be resized and cropped to the appropriate dimensions.\n\nThe pre-trained networks that will be utilized were initially trained on the ImageNet dataset, where normalization was performed separately for each color channel. For all three datasets (training, validation, and testing), it is essential to normalize the image means and standard deviations to match the expectations of the network. The mean values are set to `[0.485, 0.456, 0.406]`, and the standard deviations to `[0.229, 0.224, 0.225]`, which are calculated from the ImageNet images. By using these values, each color channel is centered around 0 and ranges from -1 to 1.\n","metadata":{}},{"cell_type":"code","source":"# Set the directory paths for training, validation, and testing data\ndata_dir = '/kaggle/input/flower-dataset/flowers'\ntrain_dir = data_dir + '/train'\nvalid_dir = data_dir + '/valid'\ntest_dir = data_dir + '/test'","metadata":{"execution":{"iopub.status.busy":"2023-08-06T15:14:29.636011Z","iopub.execute_input":"2023-08-06T15:14:29.636691Z","iopub.status.idle":"2023-08-06T15:14:29.644283Z","shell.execute_reply.started":"2023-08-06T15:14:29.636654Z","shell.execute_reply":"2023-08-06T15:14:29.643021Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Creating a normalization transform that adjusts image values to have a standard mean and deviation\nnormalize = transforms.Normalize(\n    mean=[0.485, 0.456, 0.406],  # Set the mean values for each color channel\n    std=[0.229, 0.224, 0.225]  # Set the standard deviation values for each color channel\n)\n\n# Defining transformation pipelines for different datasets (train, val, test)\ndata_transforms = {}\n\n# For training data: Randomly crop and resize to 224x224, then convert to tensor and normalize\ndata_transforms['train'] = transforms.Compose([\n    transforms.RandomResizedCrop(224),  # Crop and resize randomly\n    transforms.ToTensor(),  # Convert to tensor\n    normalize  # Normalize using the defined mean and std\n])\n\n# For validation data: Resize to 256x256, center crop to 224x224, then convert to tensor and normalize\ndata_transforms['val'] = transforms.Compose([\n    transforms.Resize(256),  # Resize\n    transforms.CenterCrop(224),  # Center crop\n    transforms.ToTensor(),  # Convert to tensor\n    normalize  # Normalize using the defined mean and std\n])\n\n# For test data: Resize to 256x256, center crop to 224x224, then convert to tensor and normalize\ndata_transforms['test'] = transforms.Compose([\n    transforms.Resize(256),  # Resize\n    transforms.CenterCrop(224),  # Center crop\n    transforms.ToTensor(),  # Convert to tensor\n    normalize  # Normalize using the defined mean and std\n])\n\n# Load the datasets using ImageFolder\nimage_datasets = {\n    'train': datasets.ImageFolder(train_dir, transform=data_transforms['train']),  # Load train data\n    'val': datasets.ImageFolder(valid_dir, transform=data_transforms['val']),  # Load validation data\n    'test': datasets.ImageFolder(test_dir, transform=data_transforms['test'])  # Load test data\n}\n\n# Create dataloaders for the datasets\ndataloaders = {\n    'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size, shuffle=True),  # Train dataloader\n    'val': torch.utils.data.DataLoader(image_datasets['val'], batch_size, shuffle=True),  # Validation dataloader\n    'test': torch.utils.data.DataLoader(image_datasets['test'], batch_size=32, shuffle=True)  # Test dataloader\n}","metadata":{"execution":{"iopub.status.busy":"2023-08-06T15:14:30.791536Z","iopub.execute_input":"2023-08-06T15:14:30.791943Z","iopub.status.idle":"2023-08-06T15:14:36.56982Z","shell.execute_reply.started":"2023-08-06T15:14:30.791908Z","shell.execute_reply":"2023-08-06T15:14:36.568788Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# A mapping of category names to labels\ncat_to_name = {\"21\": \"fire lily\", \"3\": \"canterbury bells\", \"45\": \"bolero deep blue\", \"1\": \"pink primrose\", \"34\": \"mexican aster\", \"27\": \"prince of wales feathers\", \"7\": \"moon orchid\", \"16\": \"globe-flower\", \"25\": \"grape hyacinth\", \"26\": \"corn poppy\", \"79\": \"toad lily\", \"39\": \"siam tulip\", \"24\": \"red ginger\", \"67\": \"spring crocus\", \"35\": \"alpine sea holly\", \"32\": \"garden phlox\", \"10\": \"globe thistle\", \"6\": \"tiger lily\", \"93\": \"ball moss\", \"33\": \"love in the mist\", \"9\": \"monkshood\", \"102\": \"blackberry lily\", \"14\": \"spear thistle\", \"19\": \"balloon flower\", \"100\": \"blanket flower\", \"13\": \"king protea\", \"49\": \"oxeye daisy\", \"15\": \"yellow iris\", \"61\": \"cautleya spicata\", \"31\": \"carnation\", \"64\": \"silverbush\", \"68\": \"bearded iris\", \"63\": \"black-eyed susan\", \"69\": \"windflower\", \"62\": \"japanese anemone\", \"20\": \"giant white arum lily\", \"38\": \"great masterwort\", \"4\": \"sweet pea\", \"86\": \"tree mallow\", \"101\": \"trumpet creeper\", \"42\": \"daffodil\", \"22\": \"pincushion flower\", \"2\": \"hard-leaved pocket orchid\", \"54\": \"sunflower\", \"66\": \"osteospermum\", \"70\": \"tree poppy\", \"85\": \"desert-rose\", \"99\": \"bromelia\", \"87\": \"magnolia\", \"5\": \"english marigold\", \"92\": \"bee balm\", \"28\": \"stemless gentian\", \"97\": \"mallow\", \"57\": \"gaura\", \"40\": \"lenten rose\", \"47\": \"marigold\", \"59\": \"orange dahlia\", \"48\": \"buttercup\", \"55\": \"pelargonium\", \"36\": \"ruby-lipped cattleya\", \"91\": \"hippeastrum\", \"29\": \"artichoke\", \"71\": \"gazania\", \"90\": \"canna lily\", \"18\": \"peruvian lily\", \"98\": \"mexican petunia\", \"8\": \"bird of paradise\", \"30\": \"sweet william\", \"17\": \"purple coneflower\", \"52\": \"wild pansy\", \"84\": \"columbine\", \"12\": \"colt's foot\", \"11\": \"snapdragon\", \"96\": \"camellia\", \"23\": \"fritillary\", \"50\": \"common dandelion\", \"44\": \"poinsettia\", \"53\": \"primula\", \"72\": \"azalea\", \"65\": \"californian poppy\", \"80\": \"anthurium\", \"76\": \"morning glory\", \"37\": \"cape flower\", \"56\": \"bishop of llandaff\", \"60\": \"pink-yellow dahlia\", \"82\": \"clematis\", \"58\": \"geranium\", \"75\": \"thorn apple\", \"41\": \"barbeton daisy\", \"95\": \"bougainvillea\", \"43\": \"sword lily\", \"83\": \"hibiscus\", \"78\": \"lotus lotus\", \"88\": \"cyclamen\", \"94\": \"foxglove\", \"81\": \"frangipani\", \"74\": \"rose\", \"89\": \"watercress\", \"73\": \"water lily\", \"46\": \"wallflower\", \"77\": \"passion flower\", \"51\": \"petunia\"}","metadata":{"execution":{"iopub.status.busy":"2023-08-06T15:14:36.571988Z","iopub.execute_input":"2023-08-06T15:14:36.572366Z","iopub.status.idle":"2023-08-06T15:14:36.584696Z","shell.execute_reply.started":"2023-08-06T15:14:36.572316Z","shell.execute_reply":"2023-08-06T15:14:36.583717Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Building and Training the Classifier\n\nWith the prepared data in place, it's time to construct and train the classifier. In the following steps, you'll leverage a pre-trained model from `torchvision.models` to extract image features. The goal is to create and train a novel feed-forward classifier using these extracted features.\n\nHere's what you'll be doing:\n\n* **Load a Pre-trained Network:** Begin by selecting a pre-trained network from the [`torchvision.models`](http://pytorch.org/docs/master/torchvision/models.html) module. If you're looking for a good starting point, the VGG networks are effective and easy to utilize.\n\n* **Design the Classifier:** Define a fresh, untrained feed-forward network to function as a classifier. This classifier should incorporate Rectified Linear Unit (ReLU) activations and dropout for regularization.\n\n* **Train the Classifier Layers:** Employ backpropagation to train the classifier layers. To extract meaningful features, use the pre-trained network's weights.\n\n* **Track Loss and Accuracy:** Throughout training, monitor the loss and accuracy metrics on the validation dataset. These metrics will help you identify the optimal hyperparameters.\n\nWhen conducting training, remember to update solely the weights of the feed-forward network. If everything is implemented correctly, you should achieve a validation accuracy exceeding 70%. Experiment with various hyperparameters—such as learning rate, classifier units, and epochs—to uncover the best model configuration. Keep track of these successful hyperparameters, as they'll serve as default values for the subsequent parts of the project.\n","metadata":{}},{"cell_type":"code","source":"# Load a pre-trained VGG16 model\nmodel = models.vgg16(pretrained=True)\n\n# Iterate through model parameters and set their 'requires_grad' attribute to False\n# This freezes the parameters, preventing them from being updated during training\nfor param in model.parameters():\n    param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2023-08-06T15:14:36.586454Z","iopub.execute_input":"2023-08-06T15:14:36.587289Z","iopub.status.idle":"2023-08-06T15:14:48.065376Z","shell.execute_reply.started":"2023-08-06T15:14:36.587255Z","shell.execute_reply":"2023-08-06T15:14:48.064382Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|██████████| 528M/528M [00:09<00:00, 56.4MB/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass MyNetwork(nn.Module):\n    \"\"\"\n    A simple feedforward neural network model.\n\n    Args:\n        input_size (int): Number of input features.\n        hidden_units (int): Number of hidden units in the layers.\n        drop_rate (float): Dropout rate for regularization.\n        output_size (int): Number of output classes.\n\n    Attributes:\n        layer1 (nn.Linear): First fully connected layer.\n        layer2 (nn.Linear): Second fully connected layer.\n        layer3 (nn.Linear): Third fully connected layer.\n        dropout (nn.Dropout): Dropout layer for regularization.\n        output (nn.LogSoftmax): Log-softmax activation for classification.\n    \"\"\"\n    def __init__(self, input_size, hidden_units, drop_rate, output_size):\n        super().__init__()\n\n        self.layer1 = nn.Linear(input_size, hidden_units)\n        self.layer2 = nn.Linear(hidden_units, hidden_units // 2)\n        self.layer3 = nn.Linear(hidden_units // 2, output_size)\n\n        self.dropout = nn.Dropout(drop_rate)\n\n        self.output = nn.LogSoftmax(dim=1)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the network.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after passing through the network.\n        \"\"\"\n        out = self.layer1(x)\n        out = F.relu(out)\n        out = self.dropout(out)\n\n        out = self.layer2(out)\n        out = F.relu(out)\n        out = self.dropout(out)\n\n        out = self.layer3(out)\n\n        out = self.output(out)\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-08-06T15:14:48.068012Z","iopub.execute_input":"2023-08-06T15:14:48.068369Z","iopub.status.idle":"2023-08-06T15:14:48.080168Z","shell.execute_reply.started":"2023-08-06T15:14:48.068334Z","shell.execute_reply":"2023-08-06T15:14:48.079025Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Update the classifier of the model with a custom network defined by MyNetwork,\n# using the specified input size, hidden layers, dropout rate, and output size\nmodel.classifier = MyNetwork(input_size, hidden_layers, drop_rate, output_size)","metadata":{"execution":{"iopub.status.busy":"2023-08-06T15:14:48.08179Z","iopub.execute_input":"2023-08-06T15:14:48.082151Z","iopub.status.idle":"2023-08-06T15:14:48.225896Z","shell.execute_reply.started":"2023-08-06T15:14:48.082116Z","shell.execute_reply":"2023-08-06T15:14:48.224873Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import torch\n\ndef get_train_validation(model, device, dataloaders, criterion):\n    \"\"\"\n    Calculate loss and accuracy on the validation dataset.\n\n    Args:\n        model (torch.nn.Module): The neural network model.\n        device (torch.device): The device (CPU or GPU) for computation.\n        dataloaders (dict): Dictionary containing data loaders for training and validation.\n        criterion (torch.nn.Module): The loss function.\n\n    Returns:\n        tuple: A tuple containing loss and accuracy values.\n    \"\"\"\n    loss, accuracy = 0, 0\n\n    model.to(device)  # Move the model to the specified device\n    validloader = dataloaders[\"val\"]  # Get the validation data loader\n\n    for images, labels in validloader:\n        images, labels = images.to(device), labels.to(device)\n        output = model(images)  # Generate predictions\n        loss += criterion(output, labels).item()  # Calculate loss\n        ps = torch.exp(output)\n        equity = labels == ps.max(dim=1)[1]  # Check if predictions are correct\n        accuracy += equity.type(torch.FloatTensor).mean()  # Calculate accuracy\n\n    return loss, accuracy","metadata":{"execution":{"iopub.status.busy":"2023-08-06T15:14:48.227338Z","iopub.execute_input":"2023-08-06T15:14:48.227909Z","iopub.status.idle":"2023-08-06T15:14:48.236008Z","shell.execute_reply.started":"2023-08-06T15:14:48.227875Z","shell.execute_reply":"2023-08-06T15:14:48.235023Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def train_network():\n    \"\"\"\n    Train the network using the specified data loaders and settings.\n\n    Returns:\n        torch.nn.Module: The trained model.\n    \"\"\"\n    trainloader = dataloaders[\"train\"]  # Get the training data loader\n    optimizer = optim.Adam(model.classifier.parameters(), lr=learning_rate)  # Use Adam optimizer for model's classifier parameters\n\n    model.to(device)  # Move the model to the appropriate device (CPU or GPU)\n\n    steps = 0  # Initialize the steps counter\n\n    for epoch in range(epochs):  # Loop through the specified number of epochs\n        training_loss = 0  # Initialize training loss for this epoch\n        for images, labels in trainloader:  # Loop through batches of training data\n            steps += 1  # Increment the steps counter\n            images, labels = images.to(device), labels.to(device)  # Move images and labels to the device\n            optimizer.zero_grad()  # Clear previous gradients\n            output = model.forward(images)  # Perform a forward pass\n            Loss = criterion(output, labels)  # Calculate the loss\n\n            Loss.backward()  # Perform backpropagation\n            optimizer.step()  # Update model's weights\n\n            training_loss += Loss.item()  # Accumulate training loss\n\n        model.eval()  # Set the model to evaluation mode\n        with torch.no_grad():  # Disable gradient calculation\n            loss, accuracy = get_train_validation(model, device, dataloaders, criterion)  # Calculate validation loss and accuracy\n\n        print(\n            \"Epoch: {}/{}\\nTraining Loss: {:.4f}\\nValidation Loss: {:.4f}\\nValidation Accuracy: {:.3f}%\\n\\n\".format(\n                epoch + 1,\n                epochs,\n                training_loss / len(dataloaders[\"train\"]),\n                loss / len(dataloaders[\"val\"]),\n                accuracy / len(dataloaders[\"val\"]) * 100,\n                )\n            ) # Print epoch summary\n\n        model.train()  # Set the model back to training mode\n\n    return model  # Return the trained model","metadata":{"execution":{"iopub.status.busy":"2023-08-06T15:14:48.237594Z","iopub.execute_input":"2023-08-06T15:14:48.23818Z","iopub.status.idle":"2023-08-06T15:14:48.2508Z","shell.execute_reply.started":"2023-08-06T15:14:48.238146Z","shell.execute_reply":"2023-08-06T15:14:48.249719Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Trains a neural network and returns the trained model\ntrained_model = train_network()","metadata":{"execution":{"iopub.status.busy":"2023-08-06T15:14:48.251897Z","iopub.execute_input":"2023-08-06T15:14:48.252403Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch: 1/10\nTraining Loss: 2.7753\nValidation Loss: 1.0367\nValidation Accuracy: 73.716%\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Evaluating Your Network\n\nTesting your trained network on unseen data is a vital step. This test should involve images that your network has never encountered during training or validation. Doing so provides an accurate gauge of your model's performance with entirely new images. Just as you did with validation, feed the test images through your network and calculate accuracy. A well-trained model should achieve approximately 70% accuracy on the test set.\n","metadata":{}},{"cell_type":"code","source":"def get_test_validation(model, dataloaders, device):\n    \"\"\"\n    Evaluate model accuracy on the test data.\n\n    Args:\n        model (torch.nn.Module): The neural network model.\n        dataloaders (dict): Dictionary containing dataloaders for different datasets.\n        device (str): Device to perform computations on (e.g., 'cpu' or 'cuda').\n\n    Returns:\n        tuple: A tuple containing accuracy and total data size.\n    \"\"\"\n    accuracy, data_size = 0, 0\n\n    model.to(device)  # Move the model to the specified device\n\n    model.eval()  # Set the model in evaluation mode\n    for images, labels in dataloaders['test']:\n        images, labels = images.to(device), labels.to(device)  # Move data to the same device\n        output = model.forward(images)  # Forward pass\n        ps = torch.exp(output)  # Compute the softmax probabilities\n        equity = labels == ps.max(dim=1)[1]  # Check if predictions match the labels\n        data_size += labels.size(0)  # Accumulate the total number of data points\n        accuracy += equity.type(torch.FloatTensor).sum().item()  # Accumulate correct predictions\n\n    return accuracy, data_size  # Return accuracy and total data size","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use the torch.no_grad() context to disable gradient computation\nwith torch.no_grad():\n    # Call the get_test_validation() function to get accuracy and total\n    accuracy, total = get_test_validation(model, dataloaders, device)\n\n# Print the accuracy as a percentage\nprint('Accuracy: {}%\\n'.format(100 * accuracy / total))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save the checkpoint\n\nAfter training your network, it's important to save the model for future use in making predictions. Additionally, consider saving other necessary information like the class-to-index mapping, which can be obtained from an image dataset using `image_datasets['train'].class_to_idx`. You can attach this mapping to the model as an attribute, making inference easier in the future:\n\n```python\nmodel.class_to_idx = image_datasets['train'].class_to_idx\n```","metadata":{}},{"cell_type":"code","source":"def save_model(save_dir):\n    \"\"\"\n    Save the trained model checkpoint.\n\n    Args:\n        save_dir (str): Path to the directory where the checkpoint will be saved.\n    \"\"\"\n    checkpoint = {\n        \"architecture\": 'vgg16',  # The architecture used for the model\n        \"state_dict\": model.state_dict(),  # Save the model's learned weights\n        \"class_to_idx\": image_datasets['train'].class_to_idx,  # Mapping of classes to indices\n        'model': models.vgg16(pretrained=True),  # Load a pre-trained VGG16 model\n        'classifier': model.classifier  # Save the custom classifier part of the model\n    }\n\n    torch.save(checkpoint, save_dir)  # Save the checkpoint to the specified directory","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_model('checkpoint.pth')  # Save the model to a file named 'checkpoint.pth'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading the checkpoint\n\nNow is a good time to create a function that can load a checkpoint and reconstruct the model. This will allow you to resume working on your project later without the need to retrain the network from scratch.\n","metadata":{}},{"cell_type":"code","source":"def get_loaded_model(checkpoint):\n    \"\"\"\n    Load a pre-trained model from a checkpoint.\n\n    Args:\n        checkpoint (str): The path to the checkpoint file.\n\n    Returns:\n        torch.nn.Module: The loaded pre-trained model.\n    \"\"\"\n    # Load the checkpoint\n    checkpoint = torch.load(checkpoint)\n\n    # Extract necessary components from the checkpoint\n    model = checkpoint['model']\n    model.class_to_idx = checkpoint['class_to_idx']\n    model.classifier = checkpoint['classifier']\n    model.load_state_dict(checkpoint['state_dict'])\n\n    # Freeze model parameters to prevent gradient updates during inference\n    for param in model.parameters():\n        param.requires_grad = False\n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load a previously saved model from a checkpoint file\nmodel = get_loaded_model('checkpoint.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference for Classification\n\nIn this section, you will create a function to use a trained network for inference. The goal is to pass an image through the network and predict the class of the flower shown in the image. To accomplish this, you'll define a function called `predict` that takes an image and a model as inputs. The function will then return the top *K* most likely classes along with their corresponding probabilities. The usage will look like this:\n\n```python\nprobs, classes = predict(image_path, model)\nprint(probs)\nprint(classes)\n> [0.01558163, 0.01541934, 0.01452626, 0.01443549, 0.01407339]\n> ['70', '3', '45', '62', '55']\n```\nFirst, you need to handle preprocessing the input image to make it compatible with your network.\n\n## Image Preprocessing\n\n1. Utilize the PIL library to load the image ([documentation](https://pillow.readthedocs.io/en/latest/reference/Image.html)).\n2. It's recommended to create a function that preprocesses the image, adapting it for use as input in your model. This function should process the images in the same manner they were treated during training.\n3. Start by resizing the images, making the shortest side 256 pixels while preserving the aspect ratio. This can be achieved using the `thumbnail` or `resize` methods.\n4. Following resizing, crop the central 224x224 portion of the image.\n5. Image color channels are usually encoded as integers ranging from 0 to 255. However, the model expects values between 0 and 1 in float format. You will need to perform this conversion. You can obtain a Numpy array from a PIL image using `np_image = np.array(pil_image)`.\n6. Just like during training, the network anticipates images to be normalized in a specific way. The means are `[0.485, 0.456, 0.406]`, and the standard deviations are `[0.229, 0.224, 0.225]`. Subtract the means from each color channel and then divide by the standard deviation.\n7. Lastly, PyTorch expects the color channel to be the first dimension, while it's the third dimension in both the PIL image and the Numpy array. You can rearrange dimensions using `ndarray.transpose`. The color channel needs to be the first dimension while maintaining the order of the other two dimensions.\n","metadata":{}},{"cell_type":"markdown","source":"To verify your progress, the function provided below converts a PyTorch tensor and visualizes it within the notebook. If your `process_image` function is functioning correctly, passing the output through this function should yield the original image (with the exception of cropped sections).\n","metadata":{}},{"cell_type":"code","source":"def process_image(image):\n    \"\"\"\n    Process an image for neural network input.\n\n    Args:\n        image (str): File path to the image.\n\n    Returns:\n        torch.Tensor: Transformed image as a PyTorch tensor.\n    \"\"\"\n    image = Image.open(image)  # Open the image using PIL\n\n    # Define a sequence of transformations to process the image\n    image_transform = transforms.Compose(\n        [\n            transforms.Resize(255),  # Resize the image to 255x255 pixels\n            transforms.CenterCrop(224),  # Crop the image to 224x224 pixels, centered\n            transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n            normalize  # Apply normalization (assuming 'normalize' is defined elsewhere)\n        ]\n    )\n\n    return image_transform(image)  # Apply the defined transformations and return the processed image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def imshow(image, title=None):\n    \"\"\"\n    Display an image using matplotlib.\n\n    Args:\n        image (torch.Tensor): The image to display, should be in tensor format.\n        title (str, optional): Title for the displayed image. Default is None.\n    \"\"\"\n    fig, ax = plt.subplots()  # Create a new figure and axis\n    \n    # Transpose the image from tensor format (C, H, W) to (H, W, C)\n    image = image.numpy().transpose((1, 2, 0))\n    \n    mean = np.array([0.485, 0.456, 0.406])  # Mean values for normalization\n    std = np.array([0.229, 0.224, 0.225])   # Standard deviation values for normalization\n    \n    # Normalize and clip the image pixel values to the range [0, 1]\n    image = (std * image) + mean\n    image = np.clip(image, 0, 1)\n    \n    if title:\n        ax.set_title(title)  # Set the title if provided\n    ax.imshow(image)  # Display the image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the processed image using the process_image function\nimshow(process_image('/kaggle/input/flower-dataset/flowers/test/12/image_03994.jpg'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Class Prediction\n\nTo make predictions with your model using images in the correct format, you'll need to implement a prediction function. It's a common practice to predict the top 5 (often referred to as top-$K$) most probable classes. Here's how you can achieve this:\n\n1. Calculate class probabilities.\n2. Find the top $K$ largest values among the probabilities.\n\nYou can use the [`x.topk(k)`](http://pytorch.org/docs/master/torch.html#torch.topk) method to get the top $K$ probabilities and their corresponding indices. After obtaining the indices, you'll need to convert them into actual class labels using the `class_to_idx` mapping. If you've added this mapping to the model or used an `ImageFolder` to load the data ([see here](#Save-the-checkpoint)), you can perform this conversion. Remember to invert the dictionary so that you can map from index to class as well.\n\nYour prediction function should take the path to an image and a model checkpoint as input, and return the predicted probabilities and class labels. Here's a sample code snippet:\n\n```python\nprobs, classes = predict(image_path, model)\nprint(probs)\nprint(classes)\n> [ 0.01558163  0.01541934  0.01452626  0.01443549  0.01407339]\n> ['70', '3', '45', '62', '55']\n```","metadata":{}},{"cell_type":"code","source":"def get_prediction(image_path, model, device, topk=5):\n    \"\"\"\n    Get predictions for an image using a trained model.\n\n    Args:\n        image_path (str): Path to the input image.\n        model (torch.nn.Module): The trained neural network model.\n        device (str): Device to perform computations on (e.g., 'cuda' or 'cpu').\n        topk (int): Number of top predictions to return.\n\n    Returns:\n        tuple: A tuple containing arrays of predicted probabilities and corresponding class labels.\n    \"\"\"\n    # Process the image and prepare it for prediction\n    image = process_image(image_path).unsqueeze(0).float()\n    \n    # Move model and image to the specified device\n    model, image = model.to(device), image.to(device)\n    \n    # Set the model in evaluation mode\n    model.eval()\n    \n    with torch.no_grad():\n        # Perform forward pass through the model\n        results = torch.exp(model.forward(image))\n        \n        # Get the topk predictions\n        result_top_k = results.topk(topk)\n        \n        # Extract predicted probabilities and class indices\n        probs, classes = result_top_k[0].data.cpu().numpy()[0], result_top_k[1].data.cpu().numpy()[0]\n        \n        # Convert class indices to actual class labels using class_to_idx mapping\n        idx_to_class = {key: value for value, key in model.class_to_idx.items()}\n        classes = [idx_to_class[classes[i]] for i in range(classes.size)]\n\n    return probs, classes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get predictions using the get_prediction function\nprobs, predict_classes = get_prediction(\n    '/kaggle/input/flower-dataset/flowers/test/12/image_03994.jpg', model, device\n)\n\n# Print the predicted classes\nprint(predict_classes)\n\n# Print the predicted probabilities\nprint(probs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sanity Checking\n\nAfter training a model and generating predictions, it's important to perform a sanity check to ensure the predictions make sense. Even if the testing accuracy is high, it's always a good practice to verify for any possible bugs or anomalies. To achieve this, you can utilize the `matplotlib` library to create a bar graph showing the probabilities of the top 5 predicted classes, alongside the input image. The visualization might resemble the following example:\n\n\nTo convert from the class integer encoding to actual flower names, utilize the `cat_to_name.json` file that should have been loaded earlier in the notebook. Additionally, to display a PyTorch tensor as an image, you can utilize the `imshow` function defined above.\n","metadata":{}},{"cell_type":"code","source":"def view_classify(image_path, probs, classes):\n    \"\"\"\n    Display an image with its predicted class probabilities.\n\n    Args:\n        image_path (str): Path to the image file.\n        probs (list): List of predicted class probabilities.\n        classes (list): List of predicted class indices.\n\n    Returns:\n        None\n    \"\"\"\n    # Get the names of the predicted classes using cat_to_name dictionary\n    name = [cat_to_name[i] for i in classes]\n    \n    # Display the processed image using imshow\n    imshow(process_image(image_path), cat_to_name[classes[0]])\n\n    # Create a figure with two subplots (image and graph)\n    _, (ax_img, ax_gph) = plt.subplots(nrows=2)\n    \n    # Turn off the axis for the image subplot\n    ax_img.axis('off')\n    \n    # Create a horizontal bar graph for class probabilities\n    ax_gph.barh(np.arange(len(name)), probs)\n    \n    # Set y-ticks to class indices and labels to class names\n    ax_gph.set_yticks(np.arange(len(name)))\n    ax_gph.set_yticklabels(name)\n    \n    # Invert the y-axis for a better display\n    ax_gph.invert_yaxis()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Call the function 'view_classify' to visualize classification results for a specific image\nview_classify('/kaggle/input/flower-dataset/flowers/test/12/image_03994.jpg', probs, predict_classes)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}